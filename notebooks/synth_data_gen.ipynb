{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic Data Generator \n",
        "\n",
        "**Generate solid, realistic and well structured tabular synthetic data using natural language prompts via LLMs**\n",
        "\n",
        "This notebook demonstrates how to create synthetic datasets using the **Meta-LLaMA 3.1 8B-Instruct** model from HuggingFace. You can choose another models on HuggingFace and ask which type of dataset you want then LLM powered SDG will create it.\n",
        "\n",
        "### Features\n",
        "- üéØ Natural language data specification\n",
        "- üìä Automatic JSON-to-DataFrame conversion\n",
        "- üîß 4-bit quantization for efficient GPU usage\n",
        "- üñ•Ô∏è Interactive Streamlit UI\n",
        "\n",
        "### Requirements\n",
        "- Google Colab with GPU runtime (T4 or better)\n",
        "- HuggingFace account with access to LLaMA models\n",
        "- HuggingFace API token stored in Colab secrets as `HF_TOKEN`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Install Dependencies\n",
        "\n",
        "Install PyTorch with CUDA support, HuggingFace Transformers, and Streamlit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA 12.4 support\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install HuggingFace libraries and Streamlit\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 streamlit pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Import Libraries & Authenticate with HuggingFace\n",
        "\n",
        "Import all necessary libraries and authenticate with HuggingFace using your API token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from IPython.display import display\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with HuggingFace\n",
        "# Set HF_TOKEN in Colab secrets (key icon in left sidebar)\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "print('Successfully authenticated with HuggingFace!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Configure Model\n",
        "\n",
        "Select which LLaMA model variant to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose your model variant\n",
        "MODEL_ID = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "print(f'Selected model: {MODEL_ID}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load Model with 4-bit Quantization\n",
        "\n",
        "Using 4-bit quantization to reduce memory usage while maintaining quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")\n",
        "\n",
        "print('Loading model...')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=quant_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print('Model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Define Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are a synthetic dataset generator. Generate realistic tabular data based on user descriptions.\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Output ONLY valid JSON - no explanations, no markdown, no extra text\n",
        "2. EVERY column array MUST have EXACTLY the same number of elements\n",
        "3. Column names should be lowercase with underscores (snake_case)\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\"column_name\": [\"value1\", \"value2\"], \"another_column\": [1, 2]}\n",
        "\n",
        "Remember: EXACTLY N rows per column where N is the requested count.'''\n",
        "\n",
        "\n",
        "def build_messages(user_instructions):\n",
        "    return [\n",
        "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "        {'role': 'user', 'content': user_instructions}\n",
        "    ]\n",
        "\n",
        "\n",
        "def extract_json(text):\n",
        "    decoder = json.JSONDecoder()\n",
        "    starts = [m.start() for m in re.finditer(r'{', text)]\n",
        "    for pos in reversed(starts):\n",
        "        try:\n",
        "            parsed, _ = decoder.raw_decode(text[pos:])\n",
        "            if isinstance(parsed, dict):\n",
        "                return parsed\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "\n",
        "def normalize_arrays(data, target_length):\n",
        "    normalized = {}\n",
        "    for key, values in data.items():\n",
        "        if not isinstance(values, list):\n",
        "            values = [values]\n",
        "        current_len = len(values)\n",
        "        if current_len == target_length:\n",
        "            normalized[key] = values\n",
        "        elif current_len < target_length:\n",
        "            cycles = (target_length // current_len) + 1\n",
        "            normalized[key] = (values * cycles)[:target_length]\n",
        "        else:\n",
        "            normalized[key] = values[:target_length]\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def generate_data(instructions, n_rows, temperature=0.7):\n",
        "    user_query = f'Generate EXACTLY {n_rows} rows of data.\\n\\nRequirements: {instructions.strip()}'\n",
        "    messages = build_messages(user_query)\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=1500,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    \n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response_text = decoded.split('assistant')[-1] if 'assistant' in decoded else decoded\n",
        "    parsed = extract_json(response_text)\n",
        "    \n",
        "    if parsed is None:\n",
        "        raise ValueError('Failed to parse model response as JSON')\n",
        "    \n",
        "    normalized = normalize_arrays(parsed, n_rows)\n",
        "    return pd.DataFrame(normalized)\n",
        "\n",
        "print('Core functions defined!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Generate customer data\n",
        "test_instructions = '''\n",
        "Customer data with:\n",
        "- first_name and last_name\n",
        "- age between 25 and 55\n",
        "- email addresses\n",
        "- city (US cities)\n",
        "'''\n",
        "\n",
        "print('Generating data...')\n",
        "df = generate_data(test_instructions, n_rows=10)\n",
        "print('Data generated!')\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Interactive Streamlit UI\n",
        "\n",
        "Launch an interactive interface for generating synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "st.set_page_config(page_title='SynthGen', page_icon='üß™', layout='wide')\n",
        "\n",
        "SYSTEM_PROMPT = '''You are a synthetic dataset generator. Output ONLY valid JSON.\n",
        "Format: {\"column\": [\"val1\", \"val2\"], \"other\": [1, 2]}\n",
        "EXACTLY N rows per column. No explanations.'''\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    MODEL_ID = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, quantization_config=quant_config, device_map='auto'\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "def extract_json(text):\n",
        "    decoder = json.JSONDecoder()\n",
        "    for pos in reversed([m.start() for m in re.finditer(r'{', text)]):\n",
        "        try:\n",
        "            parsed, _ = decoder.raw_decode(text[pos:])\n",
        "            if isinstance(parsed, dict): return parsed\n",
        "        except: continue\n",
        "    return None\n",
        "\n",
        "st.title('üß™ SynthGen')\n",
        "st.markdown('Generate synthetic datasets using LLaMA 3.1')\n",
        "\n",
        "with st.spinner('Loading model...'):\n",
        "    model, tokenizer = load_model()\n",
        "st.success('Model loaded!')\n",
        "\n",
        "if 'dataframe' not in st.session_state:\n",
        "    st.session_state.dataframe = pd.DataFrame()\n",
        "\n",
        "col1, col2 = st.columns([1, 2])\n",
        "\n",
        "with col1:\n",
        "    st.subheader('Configuration')\n",
        "    n_rows = st.number_input('Number of Rows', min_value=1, max_value=100, value=10)\n",
        "    temperature = st.slider('Temperature', 0.1, 1.5, 0.7, 0.1)\n",
        "    instructions = st.text_area('Describe your data', height=150)\n",
        "    \n",
        "    if st.button('Generate', type='primary'):\n",
        "        if instructions:\n",
        "            with st.spinner('Generating...'):\n",
        "                try:\n",
        "                    messages = [\n",
        "                        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                        {'role': 'user', 'content': f'Generate EXACTLY {n_rows} rows. Requirements: {instructions}'}\n",
        "                    ]\n",
        "                    inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "                    outputs = model.generate(inputs, max_new_tokens=1500, temperature=temperature, do_sample=True)\n",
        "                    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    parsed = extract_json(decoded.split('assistant')[-1] if 'assistant' in decoded else decoded)\n",
        "                    if parsed:\n",
        "                        st.session_state.dataframe = pd.DataFrame(parsed)\n",
        "                        st.success('Generated!')\n",
        "                    else:\n",
        "                        st.error('Failed to parse response')\n",
        "                except Exception as e:\n",
        "                    st.error(f'Error: {e}')\n",
        "    \n",
        "    if not st.session_state.dataframe.empty:\n",
        "        st.download_button('Download CSV', st.session_state.dataframe.to_csv(index=False), 'data.csv')\n",
        "\n",
        "with col2:\n",
        "    st.subheader('Generated Dataset')\n",
        "    if not st.session_state.dataframe.empty:\n",
        "        st.dataframe(st.session_state.dataframe, use_container_width=True)\n",
        "    else:\n",
        "        st.info('Enter instructions and click Generate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch Streamlit with ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Start Streamlit\n",
        "!nohup streamlit run streamlit_app.py --server.port 8501 > /dev/null 2>&1 &\n",
        "time.sleep(3)\n",
        "\n",
        "# Create public URL\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f'\\nüöÄ Open this URL: {public_url}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Export Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "# df.to_csv('my_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
